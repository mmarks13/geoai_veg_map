% Revision Response Letter
% Manuscript: Attention-Based Enhancement of Airborne LiDAR across Vegetated Landscapes using SAR and Optical Imagery Fusion
% File: revision1_response.tex

\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{hyperref}
\hypersetup{colorlinks=true,linkcolor=blue,urlcolor=blue}
\newcommand{\revA}{\textcolor{blue}}
\newcommand{\revB}{\textcolor{teal}}
\newcommand{\commentlabel}[1]{\textbf{Comment #1.}}
\newcommand{\response}{\textbf{Response.} }
\setlist[itemize]{topsep=2pt,itemsep=2pt,parsep=0pt}
\setlist[enumerate]{topsep=4pt,itemsep=4pt,parsep=0pt}

\begin{document}

\begin{center}
{\Large \textbf{Response to Reviewers}}\\[4pt]
Manuscript Title: \emph{Attention-Based Enhancement of Airborne LiDAR across Vegetated Landscapes using SAR and Optical Imagery Fusion}\\[2pt]
Journal: \emph{Remote Sensing} (MDPI)\\[2pt]
Date: \today
\end{center}

We thank the reviewers for their careful reading and constructive feedback. Below we reproduce each comment (in italics) followed by our point-by-point response. All manuscript changes referenced below have been incorporated into the revised submission (file: \texttt{remote\_sensing\_submission.tex}). Section / table / figure references refer to the revised manuscript. Minor grammar/style edits are not exhaustively enumerated.

For clarity, new or clarified text inserted into the manuscript is paraphrased (not always quoted verbatim) in the responses below.

\section*{Reviewer 1}

\begin{enumerate}

  \item \emph{Make the notations consistent in the manuscript text and the Figures (e.g., Figure 6: \texttt{in\_features} vs $D_{p\_in}$ / $D_{p\_out}$; difference between $D_{p\_in}$ and $D_{feat}$).}

  \response We added a concise \emph{Figure notation bridge} paragraph in the Notation Conventions subsection explicitly mapping the diagram placeholders \texttt{in\_features}, \texttt{out\_features} (and \texttt{out\_feat}) to the manuscript symbols $D_{p\_in}$ and $D_{p\_out}$. We clarified that Mermaid diagramming limitations (lack of math subscripts) necessitated the plain-text labels. We also clarified that $D_{p\_in}$/$D_{p\_out}$ denote point feature dimensionalities entering/leaving an LG-PAB stage, while $D_{p\_feat}$ refers to the working point feature embedding dimension used across blocks.

  \item \emph{Specify the exact layer setup (conv sizes, pooling window, MLP dimensions, stage naming ambiguity).}

  \response We made three changes: (i) inline parenthetical specifications for the patch tokenization stem: Conv1 ($C_{in}\rightarrow D_{token}/2$, $3\times3$, stride 1, pad 1), Conv2 ($D_{token}/2\rightarrow D_{token}$, $3\times3$, stride 1, pad 1), followed by AvgPool (kernel=stride=10) producing a $4\times4$ patch grid; (ii) inline MLP expansion factors and depth-wise convolution kernel ($D_{token}\rightarrow 4D_{token} \rightarrow D_{token}$, depth-wise $k=3$) in the Transformer block; (iii) added an Appendix table enumerating remaining architectural components: fusion projection layers, LG-PAB local/global FFNs, position generator MLP (256$\rightarrow$64$\rightarrow$32$\rightarrow$3), coordinate decoder MLP (256$\rightarrow$128$\rightarrow$64$\rightarrow$32$\rightarrow$3), positional encoding MLPs, heads, and dropout values. We also replaced the ambiguous \emph{Stage 3} wording with \emph{Transformer Encoder Block} for consistency.

  \item \emph{Line 273–274: Justification for 50k point threshold in a 10 m × 10 m tile.}

  \response We clarified that the adaptive anisotropic voxel filtering targets a \textasciitilde50k ceiling as a balance between (a) preserving fine vertical structure and (b) managing memory and compute; the ceiling is set to be at least 2× larger than the densest sparse 3DEP input clouds so supervision remains substantially denser without incurring quadratic cost escalation in global attention. The adaptive horizontal voxel enlargement (vertical edge fixed at half horizontal) continues until the cap is met, ensuring controlled density normalization across tiles.

  \item \emph{Line 306: What are the six UAVSAR channels?}

  \response We explicitly list the six fully polarimetric multi-look cross-product channels now used: \texttt{HHHH}, \texttt{HVHV}, \texttt{VVVV}, \texttt{HHHV}, \texttt{HHVV}, and \texttt{HVVV}. Extraction and resampling pipeline details (chip size, 5 m resample) are also included.

  \item \emph{Line 375: Clarify intensity and \emph{number of returns}.}

  \response We corrected the wording and clarified that the additional per-return attributes for 3DEP LiDAR include: (1) intensity (original 16-bit pulse magnitude, standardized via global mean/std), (2) return number (ordinal), and (3) number of returns (pulse multiplicity). All are standardized prior to model ingestion.

  \item \emph{Table 7: Choice of $\alpha=4$ vs. reference value 1000 for density-aware Chamfer loss.}

  \response We added explicit rationale: the prior work’s $\alpha=1000$ was tuned for small, normalized coordinate scales. Our coordinates remain in meters over larger absolute extents; using $\alpha=1000$ collapsed gradients numerically (dominating scale factors), so we re-tuned and empirically selected $\alpha=4$ to maintain loss sensitivity and stable optimization on large-scale vegetation geometry. This adaptation is now stated directly adjacent to the loss description.

  \item \emph{Figure 8 comparability of Chamfer Distance (CD) values between Input vs. Reference and Model vs. Reference given differing point counts.}

  \response We now explicitly caution (Results section and figure caption) that CD is sampling-density sensitive; the Input (sparse 3DEP vs. UAV reference) CD serves only as a qualitative sanity anchor. All quantitative model comparisons are conducted at matched upsampling (2×) against a common \textasciitilde50k-point reference, ensuring direct comparability.

  \item \emph{Related: Does differing point count magnitude affect CD comparability across scenarios?}

  \response Addressed with the clarification above: yes, raw CD magnitudes differ when sampling densities change; hence only equal-density (upsampled baseline/fusion outputs vs. standardized reference) CDs are interpreted quantitatively.

  \item \emph{Line 457–458: Canopy height change estimation details; missing tile in counts (5688 vs. 5687).}

  \response We appended a methods sentence describing net canopy height change computation (tile-level aggregation of per-point canopy heights; difference between UAV reference and temporally earlier 3DEP). One tile contained a spurious below-ground 3DEP return producing an undefined canopy change metric after quality control; it was excluded for the canopy-change correlation analysis only (N=5687). All other analyses (RQ1–RQ2) retained the full 5688 tiles. A parenthetical note documents this exclusion.

  \item \emph{Lines 468–470: Lack of improvement for canopy gains vs. losses—request for discussion.}

  \response The Discussion now elaborates that canopy loss regions benefit more from multi-modal fusion because (a) ancillary imagery captures disturbance signatures (spectral gaps, radar backscatter reduction) that align with removal of structure, whereas canopy gains often involve subtle vertical infill lacking distinctive cross-modal cues at imagery resolutions; and (b) temporal mismatch biases can over-penalize growth predictions when reference (UAV) includes emergent fine branches not inferable from older sparse LiDAR plus imagery snapshots. We note future work on higher temporal pairing and growth-focused augmentations.

  \item \emph{Appendix Figures (A1–A3): Mixed 8× (growth) vs. 2× (loss) illustrations—consistency concern.}

  \response We clarified in the Appendix note and individual captions that figures are illustrative only; all reported metrics use the production 2× models. We retained 8× growth illustrations to enhance the visibility of subtle structural emergence, explicitly noting that these are not part of quantitative evaluation.

\end{enumerate}

\section*{Reviewer 2}

\begin{enumerate}
  \item \emph{High computational complexity: Global attention over entire point cloud; parameter increase (fusion model 6.8M, +26\% over baseline).}

  \response We clarified complexity drivers in the main text. First, we documented an existing implementation detail: the 3DEP input is capped at 10,000 points per 10\,m tile (random subsample if exceeded; <1\% of tiles affected), which has bounded the global-attention sequence length and memory from the outset. This cap was already present in the publicly available code; it is now stated explicitly in Methods. With $R_{\text{up}}{=}2$, the largest sequence occurs right after expansion (at most \~20k tokens) before refinement/decoding. We implement global attention with FlashAttention to reduce memory traffic while preserving exact attention; under these bounds we observed stable runtime/memory without additional approximations. Separately, the parameter increase from 4.7M (baseline) to 6.8M (fused) arises from adding modality-specific imagery encoders and cross-attention fusion projections; learned weights depend on layer widths/head counts and are independent of per-tile point counts.

  \item \emph{High requirements for data alignment and quality; complex preprocessing; UAV:3DEP ratio > 2 may limit generalization.}

  \response We reframed the thresholds as training/benchmark curation rather than deployment requirements. The revised Methods explicitly state the UAV:3DEP > 2 and \geq\,16k UAV / \geq\,200 3DEP point thresholds were used to exclude edge-of-flight or extremely sparse tiles so the UAV reference is meaningfully denser and the 3DEP input minimally informative for stable supervision; at inference the method needs only sparse airborne LiDAR plus co-registered imagery. In the Discussion we acknowledge that perfect cross-sensor alignment is not guaranteed; we relied on established, well-orthorectified public datasets (3DEP, NAIP, UAVSAR) to minimize gross misregistration, and we chose point-to-image cross-attention with a modest proximity mask partly because it is more tolerant to small horizontal offsets than naïve feature concatenation. We did not run a formal misregistration sensitivity study and identify it as future work; we also note potential domain-shift limitations and the straightforward path to broaden training with additional high-density LiDAR from new biomes.
\end{enumerate}

\section*{Summary of Major Manuscript Changes}
\begin{itemize}
  \item Added \emph{Figure notation bridge} mapping figure labels to formal symbols.
  \item Inline architectural specifications (convolutional stem, pooling, MLP expansion) plus comprehensive Appendix layer specification table.
  \item Clarified downsampling 50k cap for UAV LiDAR reference data.
  \item Explicitly listed six UAVSAR polarimetric channels.
  \item Clarified LiDAR per-return attributes (intensity scaling, return number semantics).
  \item Added rationale for $\alpha=4$ density-aware Chamfer weighting on meter-scale data.
  \item Explicit caution on Chamfer Distance comparability across differing point densities; figure caption adjustments.
  \item Added canopy height change computation method and one-tile exclusion note (N=5687 for that analysis only).
  \item Expanded discussion on asymmetric performance (loss vs. gain) and limitations for canopy growth detection.
  \item Clarified illustrative purpose of 8× Appendix figures vs. 2× production evaluation.
  \item Documented the existing 10,000-point 3DEP input cap in Methods to bound attention context length (\textless1\% of tiles affected).
  \item Inserted a complexity paragraph (Model Architecture) outlining sequence-length bounds (cap and $R_{\text{up}}{=}2$) and FlashAttention’s memory benefits.
  \item Added a one-line note after Table~\ref{tab:model_variants} clarifying that parameter increases come from imagery encoders and fusion projections (independent of per-tile point counts).
  \item Reframed QA thresholds as training/evaluation curation filters (not inference requirements) and added an Alignment and generalization paragraph discussing misregistration, cross-attention tolerance, and domain shift.
\end{itemize}

We appreciate the reviewers’ insights, which have strengthened the clarity, transparency, and reproducibility of the manuscript. We hope the revisions satisfactorily address all concerns.

\vspace{1em}
\noindent Sincerely,\\[4pt]
Michael Marks (on behalf of all authors)

\end{document}
